{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_MNIST_Aufgaben.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gv0s-9pLS7n",
        "colab_type": "text"
      },
      "source": [
        "# DLML - GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn9loYR0EUgQ",
        "colab_type": "text"
      },
      "source": [
        "## Random Seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qrKCK6pLN54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Numpy seed.\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "\n",
        "# Set TensorFlow seed.\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQcuVC04Lb2K",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5Wr2utpLbwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import keras.backend as K\n",
        "\n",
        "from keras.layers import Input, Dense, Flatten, Reshape, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D\n",
        "from keras.layers import BatchNormalization, Dropout, ZeroPadding2D\n",
        "from keras.layers import Activation, LeakyReLU, ReLU\n",
        "\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.initializers import RandomNormal, glorot_normal\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist, fashion_mnist, cifar10\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import HTML\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.animation as animation\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1vvZ2GisHnP",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hJYppNLett4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return random noise.\n",
        "def sample_noise(size):\n",
        "    return np.random.normal(0, 1, size=size)\n",
        "    #return np.random.uniform(-1, 1, size=size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRvzVg9JivNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return datetime.\n",
        "def get_datetime(seconds):\n",
        "    return str(datetime.timedelta(seconds=seconds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mLd9kTksHsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return moving average of argument.\n",
        "def ema(data, window_size=100):\n",
        "    cumsum_vec = np.cumsum(np.insert(data, 0, 0)) \n",
        "    ma_vec = (cumsum_vec[window_size:] - cumsum_vec[:-window_size]) / window_size\n",
        "    return ma_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NENTT61di3wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return the norm of all gradients of trinable parameters (only works in training).\n",
        "def get_gradient_norm(model):\n",
        "    with K.name_scope('grad_norm'):\n",
        "        grads = K.gradients(model.total_loss, model.trainable_weights)\n",
        "        norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n",
        "    return norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPj9nMrYLbqZ",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C6p1lmCb7dJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data dimensions.\n",
        "x_dim = 28\n",
        "y_dim = 28\n",
        "channels = 1\n",
        "img_shape = (x_dim, y_dim, channels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_T1Z_JuMbS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data.\n",
        "(x_train, _), (x_test, _) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0bVmWU1M1bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add channel dimension.\n",
        "x_train = x_train.reshape(-1, x_dim, y_dim, channels)\n",
        "\n",
        "# Normalize data to interval (-1, 1).\n",
        "x_train = x_train / 255 * 2 - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fygPY-rfOe5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print data shape.\n",
        "print(x_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M6uEs_JaOYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(x_train[6].reshape(x_dim, y_dim), cmap='gray_r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdMS0cYEMbNc",
        "colab_type": "text"
      },
      "source": [
        "# GAN - Aufgaben"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng07TOXyda-W",
        "colab_type": "text"
      },
      "source": [
        "## Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqEBnPCvcU7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterations = 10000\n",
        "batch_size = 128\n",
        "latent_dim = 100\n",
        "\n",
        "init = glorot_normal() #RandomNormal(mean=0, stddev=0.02)\n",
        "optimizer = Adam(lr=0.0002, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcDV9BPMddWy",
        "colab_type": "text"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yil1t3g3Zez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build generator model.\n",
        "# ---------- Aufgabe ----------\n",
        "# Erstellen Sie eine Input layer z mit\n",
        "# dem shape latent_dim.\n",
        "z = ...\n",
        "\n",
        "x = Dense(128 * 7 * 7, kernel_initializer=init)(z)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Reshape((7, 7, 128))(x)\n",
        "\n",
        "x = Conv2D(128, kernel_size=3, strides=(1, 1), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2DTranspose(128, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2D(64, kernel_size=3, strides=(1, 1), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2DTranspose(64, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2D(channels, kernel_size=3, strides=(1, 1), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = Activation(\"tanh\")(x)\n",
        "\n",
        "generator = Model(z, x)\n",
        "generator.name = \"generator\"\n",
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anxZx3EMHUl3",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3OWyQ1fHUhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build discriminator model.\n",
        "# ---------- Aufgabe ----------\n",
        "# Erstellen Sie eine Input layer img_input mit\n",
        "# dem shape eines MNIST Bildes.\n",
        "# Schauen Sie ggf. oben nach, welchen shape die\n",
        "# MNIST Daten haben.\n",
        "img_input = ...\n",
        "\n",
        "x = Conv2D(32, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=init)(img_input)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = Conv2D(64, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = Conv2D(128, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = Conv2D(256, kernel_size=3, strides=(1, 1), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(1, kernel_initializer=init)(x)\n",
        "x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "discriminator = Model(img_input, x)\n",
        "discriminator.name = \"discriminator\"\n",
        "\n",
        "# ---------- Aufgabe ----------\n",
        "# Fügen Sie die richtige loss function (loss=...)\n",
        "# des Diskriminators. Der Diskriminator soll\n",
        "# binäre Klassifikation machen.\n",
        "discriminator.compile(loss=...,\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "discriminator.metrics_names.append(\"grad_norm\")\n",
        "discriminator.metrics_tensors.append(get_gradient_norm(discriminator))\n",
        "\n",
        "discriminator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQrsuOR7M7aN",
        "colab_type": "text"
      },
      "source": [
        "## Combined Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB2n_mTWM7Vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For the combined model we will only train the generator.\n",
        "# ---------- Aufgabe ----------\n",
        "# Frieren Sie die Gewichte des Diskriminators ein.\n",
        "...\n",
        "\n",
        "# Combined model (stacked generator and discriminator).\n",
        "combined = Model(z, discriminator(generator(z)))\n",
        "combined.name = \"combined\"\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "combined.metrics_names.append(\"grad_norm\")\n",
        "combined.metrics_tensors.append(get_gradient_norm(combined))\n",
        "\n",
        "combined.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zNvtIfKMbCs",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN8zoCKwCxes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss history.\n",
        "losses = {\n",
        "    'd_loss_real' : [],\n",
        "    'd_loss_fake' : [],\n",
        "    'd_acc_real' : [],\n",
        "    'd_acc_fake' : [],\n",
        "    'd_grad_norm_real' : [],\n",
        "    'd_grad_norm_fake' : [],\n",
        "    'd_loss' : [],\n",
        "    'd_acc' : [],\n",
        "    'd_grad_norm' : [],\n",
        "    'g_loss' : [],\n",
        "    'g_grad_norm' : []\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e-CNJtdTySj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Savely create path.\n",
        "save_path_image = \"images/\"\n",
        "if not os.path.exists(save_path_image):\n",
        "    os.mkdir(save_path_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVcYm1OxcVd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adversarial ground truths.\n",
        "# ---------- Aufgabe ----------\n",
        "# Erstellen Sie einen Vektor valid vom shape (batch_size, 1) der aus 1 besteht.\n",
        "valid = ...\n",
        "# Erstellen Sie einen Vektor fake vom shape (batch_size, 1) der aus 0 besteht.\n",
        "fake = ...\n",
        "\n",
        "# Start time measurement.\n",
        "start = time.time()\n",
        "\n",
        "# Save interval settings.\n",
        "save_interval = 50\n",
        "use_static_noise = True\n",
        "r, c = 5, 5\n",
        "static_noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "save_paths = []\n",
        "\n",
        "for iteration in range(iterations):\n",
        "\n",
        "    # ---------------------\n",
        "    #  Train Discriminator\n",
        "    # ---------------------\n",
        "\n",
        "    # Select a random half of images.\n",
        "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "    imgs = x_train[idx]\n",
        "\n",
        "    # Sample noise and generate a batch of new images.\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Train the discriminator (real classified as ones and generated as zeros).\n",
        "    # ---------- Aufgabe ----------\n",
        "    # Sie brauchen die Variablen: imgs, gen_imgs, fake, valid\n",
        "    # Geben Sie der Funktion train_on_batch die richtigen Werte um:\n",
        "    # 1. Auf echten Daten zu trainieren.\n",
        "    d_loss_real, d_acc_real, d_grad_norm_real = discriminator.train_on_batch(..., ...)\n",
        "    # 2. Auf den generierten Daten zu trainieren.\n",
        "    d_loss_fake, d_acc_fake, d_grad_norm_fake = discriminator.train_on_batch(..., ...)\n",
        "\n",
        "    # Calculate discriminator metrics (average of real and fake batches) to record.\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    d_acc = 0.5 * np.add(d_acc_real, d_acc_fake)\n",
        "    d_grad_norm = 0.5 * np.add(d_grad_norm_real, d_grad_norm_fake)\n",
        "\n",
        "    # Log discriminator metrics.\n",
        "    losses['d_loss_real'].append(d_loss_real.tolist())\n",
        "    losses['d_loss_fake'].append(d_loss_fake.tolist())\n",
        "    losses['d_acc_real'].append(d_acc_real.tolist())\n",
        "    losses['d_acc_fake'].append(d_acc_fake.tolist())\n",
        "    losses['d_grad_norm_real'].append(d_grad_norm_real.tolist())\n",
        "    losses['d_grad_norm_fake'].append(d_grad_norm_fake.tolist())\n",
        "    losses['d_loss'].append(d_loss.tolist())\n",
        "    losses['d_acc'].append(d_acc.tolist())\n",
        "    losses['d_grad_norm'].append(d_grad_norm.tolist())\n",
        "\n",
        "    # ---------------------\n",
        "    #  Train Generator\n",
        "    # ---------------------\n",
        "\n",
        "    # Sample novel noise.\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "    # Train the generator (wants discriminator to mistake images as real).\n",
        "    # ---------- Aufgabe ----------\n",
        "    # Geben Sie der Funktion train_on_batch die richtigen Werte um\n",
        "    # den Generator zu trainiere.\n",
        "    g_loss, g_grad_norm = combined.train_on_batch(..., ...)\n",
        "    \n",
        "    # Log generator metrics.\n",
        "    losses['g_loss'].append(g_loss)\n",
        "    losses['g_grad_norm'].append(g_grad_norm)\n",
        "\n",
        "    # Print progress.\n",
        "    print(\"Iteration: {:5d} [D loss: {:1.5f}, acc.: {:5.2f}%, grad norm: {:5.2f}] [G loss: {:1.5f}, grad norm: {:5.2f}]\".format(\n",
        "        iteration + 1, d_loss, 100 * d_acc, d_grad_norm, g_loss, g_grad_norm))\n",
        "\n",
        "    # Save generated image samples.\n",
        "    if (iteration + 1) % save_interval == 0 or (iteration + 1) == 1:\n",
        "        if use_static_noise:\n",
        "            noise = static_noise # Use static noise.\n",
        "        else:\n",
        "            noise = np.random.normal(0, 1, (r * c, latent_dim)) # Use new radom noise every save iteration.\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray_r')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.suptitle(\"Iteration: {:d}\".format(iteration + 1))\n",
        "        iter_save_path = \"{:s}iteration_{:d}.png\".format(save_path_image, iteration + 1)\n",
        "        save_paths.append(iter_save_path)\n",
        "        fig.savefig(iter_save_path)\n",
        "        plt.show()\n",
        "\n",
        "    # Finish time measurement.\n",
        "    if (iteration + 1) == iterations:\n",
        "        end = time.time()\n",
        "        training_time = end - start\n",
        "        print('Finished training in {:s}'.format(get_datetime(training_time)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36nnR7xADdhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Savely create path.\n",
        "save_path_training = \"training/\"\n",
        "if not os.path.exists(save_path_training):\n",
        "    os.mkdir(save_path_training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypyiG1uX0Y_X",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Create animation of generated images during trainig.\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for save_path in save_paths:\n",
        "    im = plt.imshow(mpimg.imread(save_path) , cmap='gray_r', animated=True)\n",
        "    plt.axis('off')\n",
        "    ims.append([im])\n",
        "plt.close()\n",
        "\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=50)\n",
        "ani.save('{:s}training.mp4'.format(save_path_training))\n",
        "HTML(ani.to_html5_video())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0EuYspXMa-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot generator and discriminator loss.\n",
        "d_loss = np.array(losses['d_loss'])\n",
        "g_loss = np.array(losses['g_loss'])\n",
        "\n",
        "plt.plot(d_loss, label=\"D loss\")\n",
        "plt.plot(g_loss, label=\"G loss\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D and G loss\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCC1a_FEY8c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot generator and discriminator loss (real and fake).\n",
        "d_loss = np.array(losses['d_loss'])\n",
        "g_loss = np.array(losses['g_loss'])\n",
        "d_loss_real = np.array(losses['d_loss_real'])\n",
        "d_loss_fake = np.array(losses['d_loss_fake'])\n",
        "\n",
        "plt.plot(d_loss, label=\"D loss\")\n",
        "plt.plot(g_loss, label=\"G loss\")\n",
        "plt.plot(d_loss_real, label=\"D loss (real)\")\n",
        "plt.plot(d_loss_fake, label=\"D loss (fake)\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D and G loss (real and fake)\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSoxU3XpZUUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot discriminator accuracy.\n",
        "d_acc = np.array(losses['d_acc'])\n",
        "\n",
        "d_acc_ema = ema(d_acc, window_size=100)\n",
        "\n",
        "plt.plot(d_acc, label=\"D acc\")\n",
        "plt.plot(d_acc_ema, label=\"D acc (ema)\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D acc\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k-Gz9hFvGXx8",
        "colab": {}
      },
      "source": [
        "# Plot discriminator accuracy (real and fake).\n",
        "d_acc = np.array(losses['d_acc'])\n",
        "d_acc_ema = ema(d_acc, window_size=100)\n",
        "d_acc_real = np.array(losses['d_acc_real'])\n",
        "d_acc_real_ema = ema(d_acc_real, window_size=100)\n",
        "d_acc_fake = np.array(losses['d_acc_fake'])\n",
        "d_acc_fake_ema = ema(d_acc_fake, window_size=100)\n",
        "\n",
        "plt.plot(d_acc, label=\"D acc\")\n",
        "plt.plot(d_acc_ema, label=\"D acc (ema)\")\n",
        "plt.plot(d_acc_real, label=\"D acc (real)\")\n",
        "plt.plot(d_acc_real_ema, label=\"D acc (real, ema)\")\n",
        "plt.plot(d_acc_fake, label=\"D acc (fake)\")\n",
        "plt.plot(d_acc_fake_ema, label=\"D acc (real, ema)\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D acc (real and fake)\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOY0Tt3aZTwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot generator and discriminator gradient norm.\n",
        "d_grad_norm = np.array(losses['d_grad_norm'])\n",
        "g_grad_norm = np.array(losses['g_grad_norm'])\n",
        "\n",
        "plt.plot(d_grad_norm, label=\"D grad norm\")\n",
        "plt.plot(g_grad_norm, label=\"G grad norm\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D and G grad norm\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58zw1kfgZz_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot generator and discriminator gradient norm (real and fake).\n",
        "d_grad_norm = np.array(losses['d_grad_norm'])\n",
        "g_grad_norm = np.array(losses['g_grad_norm'])\n",
        "d_grad_norm_real = np.array(losses['d_grad_norm_real'])\n",
        "d_grad_norm_fake = np.array(losses['d_grad_norm_fake'])\n",
        "\n",
        "plt.plot(d_grad_norm, label=\"D grad norm\")\n",
        "plt.plot(g_grad_norm, label=\"G grad norm\")\n",
        "plt.plot(d_grad_norm_real, label=\"D grad norm (real)\")\n",
        "plt.plot(d_grad_norm_fake, label=\"D grad norm (fake)\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D and G grad norm (real and fake)\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csyq-t0wTNvg",
        "colab_type": "text"
      },
      "source": [
        "## Generate from Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3TolMAFCcCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Savely create path.\n",
        "save_path_gen_image = \"gen_images/\"\n",
        "if not os.path.exists(save_path_gen_image):\n",
        "    os.mkdir(save_path_gen_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhmQAHteTTG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate images from random noise z.\n",
        "r, c = 5, 5\n",
        "noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "gen_imgs = generator.predict(noise)\n",
        "\n",
        "fig, axs = plt.subplots(r, c)\n",
        "cnt = 0\n",
        "for i in range(r):\n",
        "    for j in range(c):\n",
        "        axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray_r')\n",
        "        axs[i,j].axis('off')\n",
        "        cnt += 1\n",
        "\n",
        "fig.suptitle(\"Generated Images\")\n",
        "fig.savefig(\"{:s}gen_image.png\".format(save_path_gen_image))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t__KpbhFNNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize latent space travel from point A to point B.\n",
        "intermediate_steps = 100\n",
        "\n",
        "# Points in z.\n",
        "point_a = np.random.normal(0, 1, (1, latent_dim))\n",
        "point_b = np.random.normal(0, 1, (1, latent_dim))\n",
        "\n",
        "points_travel = np.array(\n",
        "    [point_a + (point_b - point_a) * x / intermediate_steps for x in range(0, 1 + intermediate_steps)]).reshape(-1, latent_dim)\n",
        "\n",
        "# Images generated from z.\n",
        "image_a = generator.predict(point_a)\n",
        "image_b = generator.predict(point_b)\n",
        "images_travel = generator.predict(points_travel)\n",
        "\n",
        "# Show origin and destination image.\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].imshow(image_a.reshape(x_dim, y_dim), cmap='gray_r')\n",
        "axs[0].axis('off')\n",
        "axs[0].title.set_text(\"Origin image\")\n",
        "axs[1].imshow(image_b.reshape(x_dim, y_dim), cmap='gray_r')\n",
        "axs[1].axis('off')\n",
        "axs[1].title.set_text(\"Destination image\")\n",
        "plt.show()\n",
        "\n",
        "# Create animation.\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for image_travel in images_travel:\n",
        "    im = plt.imshow(image_travel.reshape(x_dim, y_dim), cmap='gray_r', animated=True)\n",
        "    plt.axis('off')\n",
        "    ims.append([im])\n",
        "plt.close()\n",
        "\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=50)\n",
        "ani.save('latent_travel.mp4')\n",
        "HTML(ani.to_html5_video())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM5yMulWJB7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Travel freely through latent space.\n",
        "n_travel = 1000\n",
        "points = []\n",
        "images = []\n",
        "\n",
        "# Initial points.\n",
        "#point_prev = np.full((1, latent_dim), 0)\n",
        "point_prev = np.random.normal(0, 1, (1, latent_dim))\n",
        "point_curr = point_prev + 0.1 * np.random.uniform(-0.1, 0.1, (1, latent_dim))\n",
        "\n",
        "# Travel.\n",
        "print(\"Calculating travel points...\")\n",
        "for i in range(n_travel):\n",
        "    direction = (point_prev - point_curr) + 0.25 * np.random.uniform(-0.1, 0.1, (1, latent_dim))\n",
        "    point = point_prev + direction\n",
        "    point = point.clip(min=-3, max=3)\n",
        "    points.append(point)\n",
        "    point_curr = point_prev\n",
        "    point_prev = point\n",
        "\n",
        "points = np.array(points).reshape(-1, latent_dim)\n",
        "\n",
        "print(\"Generate images...\")\n",
        "images = generator.predict(points)\n",
        "\n",
        "# Create animation.\n",
        "print(\"Creating animation...\")\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for image in images:\n",
        "    im = plt.imshow(image.reshape(x_dim, y_dim), cmap='gray_r', animated=True)\n",
        "    plt.axis('off')\n",
        "    ims.append([im])\n",
        "plt.close()\n",
        "\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=50)\n",
        "ani.save('latent_travel_n.mp4')\n",
        "HTML(ani.to_html5_video())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xBvL40udpMPL"
      },
      "source": [
        "# GAN - Lösungen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qfc1OjE3pMPP"
      },
      "source": [
        "## Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JqCfKv7gpMPR",
        "colab": {}
      },
      "source": [
        "iterations = 10000\n",
        "batch_size = 128\n",
        "latent_dim = 100\n",
        "\n",
        "init = glorot_normal() #RandomNormal(mean=0, stddev=0.02)\n",
        "optimizer = Adam(lr=0.0002, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E2ToiSnFpMPY"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sw1WV1CBpMPb",
        "colab": {}
      },
      "source": [
        "# Build generator model.\n",
        "z = Input(shape=(latent_dim,))\n",
        "x = Dense(128 * 7 * 7, kernel_initializer=init)(z)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Reshape((7, 7, 128))(x)\n",
        "\n",
        "x = Conv2D(128, kernel_size=3, strides=(1, 1), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2DTranspose(128, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2D(64, kernel_size=3, strides=(1, 1), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2DTranspose(64, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2D(channels, kernel_size=3, strides=(1, 1), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = Activation(\"tanh\")(x)\n",
        "\n",
        "generator = Model(z, x)\n",
        "generator.name = \"generator\"\n",
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VnspdCvUpMPg"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t99cKA-zpMPi",
        "colab": {}
      },
      "source": [
        "# Build discriminator model.\n",
        "img_input = Input(shape=img_shape)\n",
        "x = Conv2D(32, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=init)(img_input)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = Conv2D(64, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = Conv2D(128, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = Conv2D(256, kernel_size=3, strides=(1, 1), padding=\"same\", kernel_initializer=init)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(1, kernel_initializer=init)(x)\n",
        "x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "discriminator = Model(img_input, x)\n",
        "discriminator.name = \"discriminator\"\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "discriminator.metrics_names.append(\"grad_norm\")\n",
        "discriminator.metrics_tensors.append(get_gradient_norm(discriminator))\n",
        "\n",
        "discriminator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-i6__79apMPm"
      },
      "source": [
        "## Combined Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bqLy_MZhpMPn",
        "colab": {}
      },
      "source": [
        "# For the combined model we will only train the generator.\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Combined model (stacked generator and discriminator).\n",
        "combined = Model(z, discriminator(generator(z)))\n",
        "combined.name = \"combined\"\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "combined.metrics_names.append(\"grad_norm\")\n",
        "combined.metrics_tensors.append(get_gradient_norm(combined))\n",
        "\n",
        "combined.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "16Za9_i6pMPr"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xOV-IEE_pMPt",
        "colab": {}
      },
      "source": [
        "# Loss history.\n",
        "losses = {\n",
        "    'd_loss_real' : [],\n",
        "    'd_loss_fake' : [],\n",
        "    'd_acc_real' : [],\n",
        "    'd_acc_fake' : [],\n",
        "    'd_grad_norm_real' : [],\n",
        "    'd_grad_norm_fake' : [],\n",
        "    'd_loss' : [],\n",
        "    'd_acc' : [],\n",
        "    'd_grad_norm' : [],\n",
        "    'g_loss' : [],\n",
        "    'g_grad_norm' : []\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LlSf8vyXpMPw",
        "colab": {}
      },
      "source": [
        "# Savely create path.\n",
        "save_path_image = \"images/\"\n",
        "if not os.path.exists(save_path_image):\n",
        "    os.mkdir(save_path_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "195l6t8hpMPz",
        "colab": {}
      },
      "source": [
        "# Adversarial ground truths.\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "#valid = np.full((batch_size, 1), 0.9)\n",
        "#fake = np.full((batch_size, 1), 0.1)\n",
        "\n",
        "# Start time measurement.\n",
        "start = time.time()\n",
        "\n",
        "# Save interval settings.\n",
        "save_interval = 50\n",
        "use_static_noise = True\n",
        "r, c = 5, 5\n",
        "static_noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "save_paths = []\n",
        "\n",
        "for iteration in range(iterations):\n",
        "\n",
        "    # ---------------------\n",
        "    #  Train Discriminator\n",
        "    # ---------------------\n",
        "\n",
        "    # Select a random half of images.\n",
        "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "    imgs = x_train[idx]\n",
        "\n",
        "    # Sample noise and generate a batch of new images.\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Train the discriminator (real classified as ones and generated as zeros).\n",
        "    d_loss_real, d_acc_real, d_grad_norm_real = discriminator.train_on_batch(imgs, valid)\n",
        "    d_loss_fake, d_acc_fake, d_grad_norm_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "    # Calculate discriminator metrics (average of real and fake batches) to record.\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    d_acc = 0.5 * np.add(d_acc_real, d_acc_fake)\n",
        "    d_grad_norm = 0.5 * np.add(d_grad_norm_real, d_grad_norm_fake)\n",
        "\n",
        "    # Log discriminator metrics.\n",
        "    losses['d_loss_real'].append(d_loss_real.tolist())\n",
        "    losses['d_loss_fake'].append(d_loss_fake.tolist())\n",
        "    losses['d_acc_real'].append(d_acc_real.tolist())\n",
        "    losses['d_acc_fake'].append(d_acc_fake.tolist())\n",
        "    losses['d_grad_norm_real'].append(d_grad_norm_real.tolist())\n",
        "    losses['d_grad_norm_fake'].append(d_grad_norm_fake.tolist())\n",
        "    losses['d_loss'].append(d_loss.tolist())\n",
        "    losses['d_acc'].append(d_acc.tolist())\n",
        "    losses['d_grad_norm'].append(d_grad_norm.tolist())\n",
        "\n",
        "    # ---------------------\n",
        "    #  Train Generator\n",
        "    # ---------------------\n",
        "\n",
        "    # Sample novel noise.\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "    # Train the generator (wants discriminator to mistake images as real).\n",
        "    g_loss, g_grad_norm = combined.train_on_batch(noise, valid)\n",
        "    \n",
        "    # Log generator metrics.\n",
        "    losses['g_loss'].append(g_loss)\n",
        "    losses['g_grad_norm'].append(g_grad_norm)\n",
        "\n",
        "    # Print progress.\n",
        "    print(\"Iteration: {:5d} [D loss: {:1.5f}, acc.: {:5.2f}%, grad norm: {:5.2f}] [G loss: {:1.5f}, grad norm: {:5.2f}]\".format(\n",
        "        iteration + 1, d_loss, 100 * d_acc, d_grad_norm, g_loss, g_grad_norm))\n",
        "\n",
        "    # Save generated image samples.\n",
        "    if (iteration + 1) % save_interval == 0 or (iteration + 1) == 1:\n",
        "        if use_static_noise:\n",
        "            noise = static_noise # Use static noise.\n",
        "        else:\n",
        "            noise = np.random.normal(0, 1, (r * c, latent_dim)) # Use new radom noise every save iteration.\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray_r')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.suptitle(\"Iteration: {:d}\".format(iteration + 1))\n",
        "        iter_save_path = \"{:s}iteration_{:d}.png\".format(save_path_image, iteration + 1)\n",
        "        save_paths.append(iter_save_path)\n",
        "        fig.savefig(iter_save_path)\n",
        "        plt.show()\n",
        "\n",
        "    # Finish time measurement.\n",
        "    if (iteration + 1) == iterations:\n",
        "        end = time.time()\n",
        "        training_time = end - start\n",
        "        print('Finished training in {:s}'.format(get_datetime(training_time)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ykETKkWpMP4",
        "colab": {}
      },
      "source": [
        "# Savely create path.\n",
        "save_path_training = \"training/\"\n",
        "if not os.path.exists(save_path_training):\n",
        "    os.mkdir(save_path_training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "EKd2jwEypMP7",
        "colab": {}
      },
      "source": [
        "# Create animation of generated images during trainig.\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for save_path in save_paths:\n",
        "    im = plt.imshow(mpimg.imread(save_path) , cmap='gray_r', animated=True)\n",
        "    plt.axis('off')\n",
        "    ims.append([im])\n",
        "plt.close()\n",
        "\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=50)\n",
        "ani.save('{:s}training.mp4'.format(save_path_training))\n",
        "HTML(ani.to_html5_video())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "igsi_ZGKpMP_",
        "colab": {}
      },
      "source": [
        "# Plot generator and discriminator loss.\n",
        "d_loss = np.array(losses['d_loss'])\n",
        "g_loss = np.array(losses['g_loss'])\n",
        "\n",
        "plt.plot(d_loss, label=\"D loss\")\n",
        "plt.plot(g_loss, label=\"G loss\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D and G loss\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WGlHI_MpMQD",
        "colab": {}
      },
      "source": [
        "# Plot generator and discriminator loss (real and fake).\n",
        "d_loss = np.array(losses['d_loss'])\n",
        "g_loss = np.array(losses['g_loss'])\n",
        "d_loss_real = np.array(losses['d_loss_real'])\n",
        "d_loss_fake = np.array(losses['d_loss_fake'])\n",
        "\n",
        "plt.plot(d_loss, label=\"D loss\")\n",
        "plt.plot(g_loss, label=\"G loss\")\n",
        "plt.plot(d_loss_real, label=\"D loss (real)\")\n",
        "plt.plot(d_loss_fake, label=\"D loss (fake)\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D and G loss (real and fake)\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-MOqc0HpMQG",
        "colab": {}
      },
      "source": [
        "# Plot discriminator accuracy.\n",
        "d_acc = np.array(losses['d_acc'])\n",
        "\n",
        "d_acc_ema = ema(d_acc, window_size=100)\n",
        "\n",
        "plt.plot(d_acc, label=\"D acc\")\n",
        "plt.plot(d_acc_ema, label=\"D acc (ema)\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D acc\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e4FShg29pMQI",
        "colab": {}
      },
      "source": [
        "# Plot discriminator accuracy (real and fake).\n",
        "d_acc = np.array(losses['d_acc'])\n",
        "d_acc_ema = ema(d_acc, window_size=100)\n",
        "d_acc_real = np.array(losses['d_acc_real'])\n",
        "d_acc_real_ema = ema(d_acc_real, window_size=100)\n",
        "d_acc_fake = np.array(losses['d_acc_fake'])\n",
        "d_acc_fake_ema = ema(d_acc_fake, window_size=100)\n",
        "\n",
        "plt.plot(d_acc, label=\"D acc\")\n",
        "plt.plot(d_acc_ema, label=\"D acc (ema)\")\n",
        "plt.plot(d_acc_real, label=\"D acc (real)\")\n",
        "plt.plot(d_acc_real_ema, label=\"D acc (real, ema)\")\n",
        "plt.plot(d_acc_fake, label=\"D acc (fake)\")\n",
        "plt.plot(d_acc_fake_ema, label=\"D acc (real, ema)\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D acc (real and fake)\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iIA7VXRQpMQM",
        "colab": {}
      },
      "source": [
        "# Plot generator and discriminator gradient norm.\n",
        "d_grad_norm = np.array(losses['d_grad_norm'])\n",
        "g_grad_norm = np.array(losses['g_grad_norm'])\n",
        "\n",
        "plt.plot(d_grad_norm, label=\"D grad norm\")\n",
        "plt.plot(g_grad_norm, label=\"G grad norm\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D and G grad norm\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "poKeYATWpMQP",
        "colab": {}
      },
      "source": [
        "# Plot generator and discriminator gradient norm (real and fake).\n",
        "d_grad_norm = np.array(losses['d_grad_norm'])\n",
        "g_grad_norm = np.array(losses['g_grad_norm'])\n",
        "d_grad_norm_real = np.array(losses['d_grad_norm_real'])\n",
        "d_grad_norm_fake = np.array(losses['d_grad_norm_fake'])\n",
        "\n",
        "plt.plot(d_grad_norm, label=\"D grad norm\")\n",
        "plt.plot(g_grad_norm, label=\"G grad norm\")\n",
        "plt.plot(d_grad_norm_real, label=\"D grad norm (real)\")\n",
        "plt.plot(d_grad_norm_fake, label=\"D grad norm (fake)\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig(\"{:s}D and G grad norm (real and fake)\".format(save_path_training))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sh0wdQwXpMQT"
      },
      "source": [
        "## Generate from Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZuyeXYtDpMQU",
        "colab": {}
      },
      "source": [
        "# Savely create path.\n",
        "save_path_gen_image = \"gen_images/\"\n",
        "if not os.path.exists(save_path_gen_image):\n",
        "    os.mkdir(save_path_gen_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "no_SoS0IpMQY",
        "colab": {}
      },
      "source": [
        "# Generate images from random noise z.\n",
        "r, c = 5, 5\n",
        "noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "gen_imgs = generator.predict(noise)\n",
        "\n",
        "fig, axs = plt.subplots(r, c)\n",
        "cnt = 0\n",
        "for i in range(r):\n",
        "    for j in range(c):\n",
        "        axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray_r')\n",
        "        axs[i,j].axis('off')\n",
        "        cnt += 1\n",
        "\n",
        "fig.suptitle(\"Generated Images\")\n",
        "fig.savefig(\"{:s}gen_image.png\".format(save_path_gen_image))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BkeckfOXpMQa",
        "colab": {}
      },
      "source": [
        "# Visualize latent space travel from point A to point B.\n",
        "intermediate_steps = 100\n",
        "\n",
        "# Points in z.\n",
        "point_a = np.random.normal(0, 1, (1, latent_dim))\n",
        "point_b = np.random.normal(0, 1, (1, latent_dim))\n",
        "\n",
        "points_travel = np.array(\n",
        "    [point_a + (point_b - point_a) * x / intermediate_steps for x in range(0, 1 + intermediate_steps)]).reshape(-1, latent_dim)\n",
        "\n",
        "# Images generated from z.\n",
        "image_a = generator.predict(point_a)\n",
        "image_b = generator.predict(point_b)\n",
        "images_travel = generator.predict(points_travel)\n",
        "\n",
        "# Show origin and destination image.\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].imshow(image_a.reshape(x_dim, y_dim), cmap='gray_r')\n",
        "axs[0].axis('off')\n",
        "axs[0].title.set_text(\"Origin image\")\n",
        "axs[1].imshow(image_b.reshape(x_dim, y_dim), cmap='gray_r')\n",
        "axs[1].axis('off')\n",
        "axs[1].title.set_text(\"Destination image\")\n",
        "plt.show()\n",
        "\n",
        "# Create animation.\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for image_travel in images_travel:\n",
        "    im = plt.imshow(image_travel.reshape(x_dim, y_dim), cmap='gray_r', animated=True)\n",
        "    plt.axis('off')\n",
        "    ims.append([im])\n",
        "plt.close()\n",
        "\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=50)\n",
        "ani.save('latent_travel.mp4')\n",
        "HTML(ani.to_html5_video())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RwaeOIbFpMQc",
        "colab": {}
      },
      "source": [
        "# Travel freely through latent space.\n",
        "n_travel = 1000\n",
        "points = []\n",
        "images = []\n",
        "\n",
        "# Initial points.\n",
        "#point_prev = np.full((1, latent_dim), 0)\n",
        "point_prev = np.random.normal(0, 1, (1, latent_dim))\n",
        "point_curr = point_prev + 0.1 * np.random.uniform(-0.1, 0.1, (1, latent_dim))\n",
        "\n",
        "# Travel.\n",
        "print(\"Calculating travel points...\")\n",
        "for i in range(n_travel):\n",
        "    direction = (point_prev - point_curr) + 0.25 * np.random.uniform(-0.1, 0.1, (1, latent_dim))\n",
        "    point = point_prev + direction\n",
        "    point = point.clip(min=-3, max=3)\n",
        "    points.append(point)\n",
        "    point_curr = point_prev\n",
        "    point_prev = point\n",
        "\n",
        "points = np.array(points).reshape(-1, latent_dim)\n",
        "\n",
        "print(\"Generate images...\")\n",
        "images = generator.predict(points)\n",
        "\n",
        "# Create animation.\n",
        "print(\"Creating animation...\")\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for image in images:\n",
        "    im = plt.imshow(image.reshape(x_dim, y_dim), cmap='gray_r', animated=True)\n",
        "    plt.axis('off')\n",
        "    ims.append([im])\n",
        "plt.close()\n",
        "\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=50)\n",
        "ani.save('latent_travel_n.mp4')\n",
        "HTML(ani.to_html5_video())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDx2P4DCpPZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}